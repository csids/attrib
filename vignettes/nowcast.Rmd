---
title: "Nowcasting with Attrib"
author: "Aurora Hofman"
date: "2020-07-21"
output: rmarkdown::html_vignette
figure_width: 6
figure_height: 4
vignette: >
 %\VignetteIndexEntry{Nowcasting with Attrib}
 %\VignetteEncoding{UTF-8}
 %\VignetteEngine{knitr::rmarkdown}
editor_options:
  chunk_output_type: console
chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(attrib)
library(ggplot2)

```
The nowcasting functions in `attrib` are made to correct for delay in registration. This vignette will go through how to use:

- `nowcast_aggregate` to change registration data to aggregated weekly data,
- `nowcast` to correct for the delay in registration, 
- `nowcast_eval` to evaluate the estimates made by `nowcast`, 
- `baseline_est` to compare the estimate made by nowcast to what is expected. 

To do so we will use the delay in mortality registration in Nroway as a example. All the data used in this example are fake data. 

# Aggregateing registration data using `nowcast_aggregate`. 

For this example some fake mortality data has been generated. 

```{r}
mortality_data_raw <- attrib::data_fake_nowcasting_county_raw
head(mortality_data_raw)

```

As we can see this data set contains only date of registration "doe" and date of registration "dor". To use `nowcast_aggregate` these two column must exist. The function also takes in the aggregation date and the number of weeks before the aggregation date we want to calculate the percentage of total registrations and the total number of registered events.

```{r}
aggregation_date <- as.Date("2020-01-01")
n_week <- 10
pop_data<- fhidata::norway_population_by_age_cats(cats = list(c(1:120)))[location_code %in% unique(fhidata::norway_locations_b2020$county_code)]
mortality_data_aggregated <- nowcast_aggregate(mortality_data_raw,
                                               aggregation_date = aggregation_date, 
                                               n_week = n_week, pop_data = pop_data )
``` 


```{r}
tail(mortality_data_aggregated[location_code == "county03"])
```

As we can see we have now generated a datatable containting the number of registered deaths per week, the percentage of registraded deaths and the number of deaths registrated within all weeks up to "n_week". 

We also note that for the last weeks in the dataset we do not have information about what happens in the future, hence these are set to "NA". 

We can now for example plot the percentages og registrations after $k$ weeks and use this to evaluate how many weeks we need to correct. 

```{r fig.height=4, fig.width=6, warning = FALSE}
q <- ggplot(data = mortality_data_aggregated[location_code == "county03"], aes(x = cut_doe, y = p0_2))
q <- q + geom_point() 
q <- q + scale_y_continuous("Percentage of deaths registered within 2 weeks", limits = c(0,1))
q <- q + theme(axis.title.x=element_blank(), axis.text.x = element_text(angle=90))

q
```

```{r fig.height=4, fig.width=6, warning = FALSE}
q <- ggplot(data = mortality_data_aggregated[location_code == "county03"], aes(x = cut_doe, y = p0_3))
q <- q + geom_point()
q <- q + scale_y_continuous("Percentage of deaths registered within 2 weeks", limits = c(0.5,1))
q <- q + theme(axis.title.x=element_blank(), axis.text.x = element_text(angle=90))
q
```

```{r fig.height=4, fig.width=6, warning = FALSE}
q <- ggplot(data = mortality_data_aggregated[location_code == "county03"], aes(x = cut_doe, y = p0_5))
q <- q + geom_point()
q <- q + scale_y_continuous("Percentage of deaths registered within 2 weeks", limits = c(0.8,1))
q <- q + theme(axis.title.x=element_blank(), axis.text.x = element_text(angle=90))
q
```


We can see that for this fake dataset we have around $80 \%$ of the registrations after 3 weeks and a $100 \% $ after 5 weeks.

# Using nowcast to correct n_deaths

When we have a data frame either generated by `nowcast_aggregate`, or on the same form as shown above, we can use `nowcast` to correct for the delay in registration. We set how many week we want to adjust and how many weeks we want to train the model on. Note that we assume all the weeks before the weeks we state to be adjusted to be the true data. In other words we assume 100 % of the deaths to be a part of the data. If in doubt set "n_week_adjust" a bit higher. 

In our case as seen from the plots after 5 weeks almost 100 % of the data are registered and we therefor chose to correct only 5 weeks. 

We use the default functions for "nowcast_correction_fn" and "nowcast_correction_sim_fn", however these can be manually set as long as they full fill the conditions for the respective functions. 

```{r}
n_week_adjusting <- 5
n_week_training <- 52
offset <- "log(pop)"
date_0 <- as.Date("2019-12-16")
nowcast_object <- nowcast_exp(mortality_data_aggregated,
  offset,
  n_week_adjusting,
  n_week_training,
  date_0,
  nowcast_correction_fn = nowcast_correction_fn_negbin_mm,
  nowcast_correction_sim_fn = nowcast_correction_sim_neg_bin)
```

`nowcast` returns two datasets. One beeing the original dataset with the corrected data added and the other containing simulations for the predicted value of ncor so one can make credible intervals. 

```{r}
tail(nowcast_object$data)
```

```{r}
tail(nowcast_object$data_sim)
```


# Evaluating the `nowcast`estimates. 

```{r}
nowcast_eval_object <- nowcast_eval(nowcast_object, n_week_adjusting)
```

We can evaluate all the residual plots to make sure there is no bias. Here we see the residualplot for n corrected in week 2. 
```{r fig.height=4, fig.width=6}
nowcast_eval_object[[2]]$std_residualplot
```

To evaluate the correction we use the absolute error ("abs_error"), $R^2$ ("R_squared"), mean square error ("MSE"), and the root mean square error ("RMSE"). 
```{r}
model_data <- data.table::data.table(
  ncor = 0:(n_week_adjusting-1)
)



for (i in 0:(n_week_adjusting-1)){
  model_data[i+1, abs_error := nowcast_eval_object[[i+1]]$abs_error]
  model_data[i+1, R_squared := nowcast_eval_object[[i+1]]$R_squared]
  model_data[i+1, MSE := nowcast_eval_object[[i+1]]$MSE]
  model_data[i+1, RMSE := nowcast_eval_object[[i+1]]$RMSE]

}

model_data

```


# Basline estimation of mortality

We use the aggregated mortality data to make an estimate of expected mortality for each week and year using `baseline_est`. 

```{r}

data_train <- mortality_data_aggregated[cut_doe< "2019-06-30"]
data_predict <- mortality_data_aggregated


n_sim <- 1000

response <- "n_death"
fixef <- "1 + sin(2 * pi * (week) / 53) + cos(2 * pi * (week ) / 53) + year"
ranef <- "(1|location_code)"
offset <- "log(pop)"


base_line <- baseline_est(data_train, data_predict, fixef = fixef, ranef = ranef, response = response, offset = offset)
```


We then use both data tables generated from the `nowcast` functionand compare the results to the baseline estimate. 
```{r}
 nowcast_data <- data.table::as.data.table(nowcast_object$data)
 nowcast_sim <- data.table::as.data.table(nowcast_object$data_sim)
```


```{r}
# Quantile functins
q025 <- function(x){
  return(quantile(x, 0.025))
}
q975 <- function(x){
  return(quantile(x, 0.975))
}
```


We need to aggregate the simulated data form `nowcast`. 
```{r}
col_names <- colnames(nowcast_sim)
data.table::setkeyv(nowcast_sim,
                    col_names[!col_names %in% c("sim_value", "sim_id")])

aggregated_nowcast_sim<- nowcast_sim[,
                                   unlist(recursive = FALSE,
                                          lapply(.(median = median, q025 = q025, q975 = q975),
                                                                    function(f) lapply(.SD, f)
                                   )),
                                   by = eval(data.table::key(nowcast_sim)),
                                   .SDcols = c("sim_value")]
aggregated_nowcast_sim

```



```{r}
#nowcast_data[, q025.sim_value := ncor]
nowcast_data[aggregated_nowcast_sim, on = .(cut_doe,location_code), q025.sim_value := q025.sim_value]

#nowcast_data[, q975.sim_value := ncor]
nowcast_data[aggregated_nowcast_sim, on = .(cut_doe,location_code), q975.sim_value := q975.sim_value]

```

Finally we can plot them all together to see if the mortality falls withing the expected values. 

```{r fig.height=4, fig.width=6}
q <- ggplot(base_line$aggregated[year == "2019" & location_code == "county03"],
            aes(x = week,
                y = median.sim_value))
q <- q + geom_ribbon(data = base_line$aggregated[year== 2019 & location_code == "county03"],
                     aes(x = week, ymin=q025.sim_value,
                         ymax=q975.sim_value,
                         colour = "Baseline estimate",
                         fill ="Baseline estimate"),
                     alpha=0.5)
q <- q + geom_line(data = nowcast_data[year == "2019"& location_code == "county03"],
                   aes(x = week, y = ncor,  colour = "Number of deaths corrected" ))
q <- q + geom_line(data = nowcast_data[location_code == "county03"][(nrow(nowcast_data[location_code == "county03"])-n_week_adjusting + 1):nrow(nowcast_data[location_code == "county03"])],
                   aes(x = week, y = q025.sim_value ,colour = "Credible intervall for n corrected" ),
                   linetype = "dashed")
q <- q + geom_line(data = nowcast_data[location_code == "county03"][(nrow(nowcast_data[location_code == "county03"])-n_week_adjusting +1):nrow(nowcast_data[location_code == "county03"])],
                   aes(x = week, y = q975.sim_value, colour = "Credible intervall for n corrected" ),
                   linetype = "dashed")
q <- q + scale_y_continuous(name = "Number of deaths")
q <- q + guides(fill = FALSE)
q
```

Had this been true data we would have had less than expected mortality for week 30 until 45. 
